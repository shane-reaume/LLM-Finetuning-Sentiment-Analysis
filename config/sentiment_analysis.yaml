# Sentiment Analysis Model Configuration

# Data settings
data:
  dataset_name: "imdb"  # Using IMDB movie reviews dataset from Hugging Face
  train_split: "train"
  validation_split: "test"
  max_length: 128  # Truncate reviews to 128 tokens to save memory
  cache_dir: "./data/processed"
  # Set to true to use augmented dataset with challenging examples
  use_augmented_dataset: false
  # Path to augmented dataset (relative to project root)
  augmented_dataset_path: "data/processed/imdb_augmented"

# Model settings
model:
  name: "distilbert-base-uncased"  # Smaller model that fits on 8GB GPU
  save_dir: "./models/sentiment"

# Training settings
training:
  batch_size: 16
  gradient_accumulation_steps: 2  # Effective batch size of 32
  num_train_epochs: 3
  learning_rate: 2.0e-5  # Using proper YAML float notation
  weight_decay: 0.01
  warmup_steps: 500
  save_steps: 1000
  save_total_limit: 2  # Only keep the 2 most recent checkpoints to save disk space
  logging_dir: "./logs/sentiment"
  evaluation_strategy: "steps"
  eval_steps: 500
  fp16: true  # Mixed precision training to save memory

# Testing settings
testing:
  test_examples_file: "data/processed/sentiment_test_examples.json"
  test_batch_size: 16
  evaluation_metrics:
    - accuracy
    - f1
  confidence_threshold: 0.7  # Threshold for high confidence predictions
  performance_test:
    latency_threshold_ms: 100  # Maximum acceptable inference time per example
    batch_size: 32